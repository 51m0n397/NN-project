{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Demucs training l2.ipynb","provenance":[{"file_id":"1WimGsrmBTz45gQUhbbcBMBJqyuMbK-Fj","timestamp":1625140522077},{"file_id":"19ROOFcj1iayoWEGQoWf0Wkec9_0Lk8e2","timestamp":1621004895745},{"file_id":"1agh-Swz-m3AB58eCgm3kalTg2MgctemT","timestamp":1619711207564},{"file_id":"1A2JetITVtQJhT1skJt-tMCuLBru_g7Kh","timestamp":1618820787894},{"file_id":"1p79POHr4O_DQjxWovmAQ4V0JdsKKBAWE","timestamp":1618157846696},{"file_id":"1TntV_6wXzzWWv-J_Px75-VL7B3w0ev00","timestamp":1617102968676}],"collapsed_sections":[],"mount_file_id":"1fB3r3BVemcqMrLInzpa9Ctykuez4pB-L","authorship_tag":"ABX9TyNz45IIjvrBkrW4esCXtx0V"},"kernelspec":{"display_name":"Python 3","name":"python3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"M8ZAT2unzD3V"},"source":["Installs"]},{"cell_type":"code","metadata":{"id":"gUNX8l8CzGWS"},"source":["!pip install musdb\n","!pip install museval\n","!apt install -y ffmpeg\n","!pip install stempeg"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MQZLxGeRZCX-"},"source":["Imports"]},{"cell_type":"code","metadata":{"id":"LK8arCfJ40MY"},"source":["import math\n","import torch as th\n","from torch import nn\n","from torch.utils.data import DataLoader\n","import musdb\n","import museval\n","from IPython.display import Audio, display\n","from pathlib import Path\n","from google.colab import drive\n","import numpy as np\n","import random\n","import os"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oZEo4GAQqMh3"},"source":["Copy dataset"]},{"cell_type":"code","metadata":{"id":"He5qSbDo29bi"},"source":["'''\n","I've downloaded and extracted the musdb dataset from \n","https://zenodo.org/record/1117372#.YKTWk5MzbAI into my google drive.\n","Here I'm copying it inside the colab VM for faster loading in the rest of the\n","code.\n","'''\n","drive.mount(\"/content/drive\")\n","!rsync -rah --info=progress2 /content/drive/MyDrive/musdb /content"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sLMsRyIqxxYj"},"source":["Parameters"]},{"cell_type":"code","metadata":{"id":"G_6C_qKmxy3M"},"source":["# Seed for the random generators\n","seed = 42\n","\n","# Parameters of the neural network\n","channels = 64\n","depth = 6\n","encoder_kernel_1 = 8\n","encoder_stride_1 = 4\n","encoder_kernel_2 = 1\n","encoder_stride_2 = 1\n","decoder_kernel_1 = 3\n","decoder_stride_1 = 1\n","decoder_kernel_2 = 8\n","decoder_stride_2 = 4\n","lstm_layers = 2\n","growth = 2\n","rescale = 0.1\n","\n","# Parameters of the dataset generation\n","samplerate = 44100\n","sample_length = 10\n","samples_per_minute = 1.25\n","shift_seconds = 1\n","workers = os.cpu_count()\n","\n","# Parameters of training\n","batch_size = 8\n","epochs = 240\n","augmentation = True\n","learning_rate = 3e-4\n","loss_function = \"L2\" # L1, L2\n","apply_shifts = 10\n","\n","# Path to the saved model checkpoint\n","model_path = \"drive/MyDrive/checkpoint/model-l2.pt\"\n","\n","# Path to the folder where to save the estimates\n","estimates_path = \"drive/MyDrive/estimates-l2/\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"27_3ons9evdR"},"source":["Init"]},{"cell_type":"code","metadata":{"id":"0Dgo_Yxcew0x"},"source":["# Initializing the random generators and the device\n","th.manual_seed(seed)\n","random.seed(seed)\n","np.random.seed(seed)\n","\n","device = th.device('cpu')\n","if th.cuda.is_available():\n","    device = th.device('cuda')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"essodVtzZFjX"},"source":["Model definition"]},{"cell_type":"code","metadata":{"id":"-K1XE4ZE44Hw"},"source":["class Model(nn.Module):\n","  def __init__(self):\n","    super().__init__()\n","\n","    self.audio_channels = 2\n","    self.tracks = 4\n","\n","    C_in = self.audio_channels\n","    C_out = channels\n","\n","    self.encoder = nn.ModuleList()\n","    for i in range(0, depth):\n","      if i > 0:\n","        C_in = C_out\n","        C_out = growth * C_in\n","      self.encoder.append(nn.Sequential(\n","        nn.Conv1d(C_in, C_out, encoder_kernel_1, encoder_stride_1), \n","        nn.ReLU(), \n","        nn.Conv1d(C_out, 2*C_out, encoder_kernel_2, encoder_stride_2), \n","        nn.GLU(dim=1)\n","      ))  \n","\n","    self.lstm = nn.LSTM(hidden_size=C_out, input_size=C_out, bidirectional=True, \n","                        num_layers=lstm_layers)\n","    self.linear = nn.Linear(2 * C_out, C_out)\n","\n","    self.decoder = nn.ModuleList()\n","    for i in range(0, depth):\n","      C_in = C_out\n","      C_out = int(C_in / growth)\n","      if i == depth-1:\n","        C_out = self.audio_channels*self.tracks\n","      blocks = [\n","        nn.Conv1d(C_in, 2*C_in, decoder_kernel_1, decoder_stride_1), \n","        nn.GLU(dim=1),\n","        nn.ConvTranspose1d(C_in, C_out, decoder_kernel_2, decoder_stride_2)\n","      ]\n","      if i < depth-1: blocks.append(nn.ReLU())\n","      self.decoder.append(nn.Sequential(*blocks))\n","\n","  def forward(self, x):\n","    encoder_out = [x]\n","    for encode in self.encoder:\n","      x = encode(x)\n","      encoder_out.append(x)\n","\n","    # conv1d/ConvTranspose1d takes in input and gives in output \n","    # [batch, channels, length] while lstm/linear wants \n","    # [length, batch, size (i.e., channels)]\n","    x = x.permute(2, 0, 1)\n","    x = model.lstm(x)[0]\n","    x = model.linear(x)\n","    x = x.permute(1, 2, 0)\n","\n","    for decode in model.decoder:  \n","      skip = encoder_out.pop()\n","      skip = th.narrow(skip, 2, (skip.size(2)-x.size(2)) // 2, x.size(2))\n","      x = x + skip\n","      x = decode(x)\n","\n","    x = x.view(x.size(0), self.tracks, self.audio_channels, x.size(-1))\n","    return x\n","    \n","model = Model()\n","model.to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0QAlcWTecdWw"},"source":["'''\n","As stated in the original source code it returns \"the nearest valid length to \n","use with the model so that there is no time steps leftover in convolutions\".\n","The number of samples of input mixtures must be \"valid\", i.e., such that\n","\"all convolution windows are full\". This \"prevents hard to debug mistake with \n","the prediction being shifted compared to the input mixture\".\n","'''\n","\n","def valid_length(length):\n","  for _ in range(depth):\n","    length = math.ceil((length - encoder_kernel_1) / encoder_stride_1) + 1\n","    length = max(1, length)\n","    length += decoder_kernel_1 - 1\n","  for _ in range(depth):\n","    length = (length - 1) * encoder_stride_1 + encoder_kernel_1\n","  return int(length)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xOezZEKehUsy"},"source":["'''\n","As explained in the paper, the demucs model is not time equivariant. The\n","apply_model function works around this problem by using the \"shift trick\", i.e.,\n","it \"samples S random shifts of an input mixture x and averages the predictions \n","of the model for each, after having applied the opposite shift\".\n","The function also allows splitting the mixture in chunks of 10 seconds before \n","applying the model to reduce the amount of VRAM needed.\n","'''\n","\n","def apply_model(model, mix, shifts=None, split=False):\n","  def apply(mix):\n","    length = mix.size(-1)\n","    delta = valid_length(length) - length\n","    padded = nn.functional.pad(mix, (delta // 2, delta - delta // 2))\n","    with th.no_grad():\n","      output = model(padded.unsqueeze(0))[0]\n","    return th.narrow(output, -1, (output.size(-1) - length) // 2, length)\n","\n","  def shift(mix):\n","    length = mix.size(-1)\n","    max_shift = int(samplerate / 2)\n","    padded = nn.functional.pad(mix, (max_shift, max_shift))\n","    offsets = [random.randint(0, max_shift) for i in range(shifts)]\n","    output = 0\n","    for offset in offsets:\n","      shifted = th.narrow(padded, -1, offset, length + max_shift)\n","      shifted_output = apply(shifted)\n","      output += th.narrow(shifted_output, -1, max_shift - offset, length)\n","    return output / shifts\n","\n","  def split(mix):\n","    length = mix.size(-1)\n","    chunk_len = samplerate * 10\n","    padded_len = math.ceil(length/chunk_len)*chunk_len\n","    padded = nn.functional.pad(mix, (0, padded_len - length))\n","\n","    chunks = th.split(padded, chunk_len, dim=-1)\n","    chunks_output = map(lambda x: shift(x) if shifts else apply(x), chunks)\n","    output = th.cat(list(chunks_output), dim=-1)\n","\n","    return th.narrow(output, -1, 0, length)\n","\n","  if split: return split(mix)\n","  return shift(mix) if shifts else apply(mix)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0v95T1MEZIOq"},"source":["Rescaling"]},{"cell_type":"code","metadata":{"id":"_O-TinvfWj7h"},"source":["# Weight rescaling at initialization\n","for layer in model.modules():\n","  if isinstance(layer, (nn.Conv1d, nn.ConvTranspose1d)):\n","    alpha = math.sqrt(layer.weight.std().detach() / rescale)\n","    layer.weight.data /= alpha\n","    if layer.bias is not None:\n","      layer.bias.data /= alpha"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"w72fFo1BbOro"},"source":["Dataset"]},{"cell_type":"code","metadata":{"id":"qGz3RLZLW-wS"},"source":["'''\n","This cell extracts and loads in memory the training dataset. It first computes \n","the mean and the standard deviation of each training track in the musDB database \n","and then extracts random chunks of length \"sample_length+shift_seconds\" from \n","every track and applies z-score normalization using the previously computed mean \n","and standard deviation.\n","'''\n","\n","trainingMusdb = musdb.DB(Path(\"musdb\"), subsets=[\"train\"], split=\"train\")\n","\n","train_meta = []\n","for i, track in enumerate(trainingMusdb.tracks, 0):\n","  print('\\r', '%d/%d' % (i + 1, len(trainingMusdb)), end = \"\")\n","  mono = np.mean(track.audio, axis=1)\n","  train_meta.append({\"mean\": mono.mean(), \"std\": mono.std()})\n","  del mono\n","  \n","sample_duration = valid_length(sample_length*samplerate)/samplerate+shift_seconds\n","\n","training_set = []\n","\n","for i, track in enumerate(trainingMusdb.tracks, 0):\n","  track.chunk_duration = sample_duration\n","\n","  print('\\r', '%d/%d' % (i + 1, len(trainingMusdb)), end = \"\")\n","\n","  for start in [random.randint(0, int(track.duration-sample_duration)-1) \n","                for j in range(max(1, int(samples_per_minute*track.duration/60)))]:\n","    track.chunk_start = start\n","    song = track.stems.transpose(0, 2, 1)\n","    song = (song - train_meta[i][\"mean\"]) / train_meta[i][\"std\"]\n","    training_set.append(th.from_numpy(song).float())\n","\n","trainingLoader = DataLoader(training_set, batch_size=batch_size, \n","                            num_workers=workers, shuffle=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DXbxwJB52sx2"},"source":["'''\n","The validation tracks are instead loaded from the disk every time to reserve as \n","much memory as possible for the training dataset. As shown later in the training\n","loop, the validation loss is computed only once every 10 epochs and using only \n","the first 30 seconds from each track.\n","'''\n","validationMusdb = musdb.DB(Path(\"musdb\"), subsets=[\"train\"], split=\"valid\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZoNguEuUIT-b"},"source":["Augmentation"]},{"cell_type":"code","metadata":{"id":"cc3fgYMQ2bDt"},"source":["# Randomly shifts the audio in time.\n","def shift(batch, time):\n","  l = []\n","  for track in batch:\n","    length = track.size(-1) - time\n","    offsets = [random.randrange(time) for i in range(track.size(0))]\n","    track = [th.narrow(track[i], -1, offsets[i], length) for i in range(track.size(0))]\n","    l.append(th.stack(track))\n","  return th.stack(l)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pGJeIgWo0EK0"},"source":["# Randomly flips the audio channels.\n","def flipChannels(batch):\n","  if batch.size(2) == 2:\n","    for i, track in enumerate(batch):\n","      for j, target in enumerate(track):\n","        if bool(random.getrandbits(1)):\n","          batch[i,j] = batch[i,j].flip(0)\n","  return batch"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bOig4jHH-6Mp"},"source":["# Randomly flips the sign.\n","def flipSign(batch):\n","  for i, track in enumerate(batch):\n","    for j, target in enumerate(track):\n","      if bool(random.getrandbits(1)):\n","        batch[i,j] = batch[i,j] * -1\n","  return batch"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BK3a3fWAAdjN"},"source":["# Shuffles the sources within the batch.\n","def remix(batch):\n","  tracks, targets, channels, length = batch.size()\n","  permutation = th.stack([th.randperm(tracks, device=batch.device) \n","                          for target in range(targets)]).transpose(0, 1)\n","  return batch.gather(0, permutation.view(tracks, targets, 1, 1)\n","              .expand(-1, -1, channels, length))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2C4d2R5Jx2o1"},"source":["Training"]},{"cell_type":"code","metadata":{"id":"jSrwuuxhnEE9"},"source":["last_epoch = 0\n","optimizer = th.optim.Adam(model.parameters(), lr=learning_rate)\n","\n","criterion = nn.L1Loss()\n","if (loss_function == \"L2\"): criterion = nn.MSELoss()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AsfYOPMMnERW"},"source":["# Execute this cell to load a checkpoint\n","try:\n","  checkpoint = th.load(Path(model_path))\n","  model.load_state_dict(checkpoint['model_state_dict'])\n","  optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","  last_epoch = checkpoint['epoch']\n","except:\n","  print(\"Start from 0\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"e3J2mGM--fD3"},"source":["'''\n","This cell trains the network and saves a checkpoint in the google drive \n","every 10 epochs.\n","'''\n","for epoch in range(last_epoch, epochs):\n","  model.train()\n","  train_loss_sum = 0\n","  train_loss = 0\n","\n","  for i, batch in enumerate(trainingLoader, 0):    \n","    batch = batch.to(device)\n","\n","    sources = batch[:, 1:]\n","    if augmentation:\n","      sources = remix(shift(flipChannels(flipSign(sources)), \n","                            shift_seconds*samplerate))\n","    else:\n","      sources = shift(sources, shift_seconds*samplerate)\n","    input = sources.sum(dim=1)\n","    estimated_output = model(input)\n","    expected_output = th.narrow(sources, 3, \n","                                (sources.size(3)-estimated_output.size(3)) // 2, \n","                                estimated_output.size(3))\n","\n","    loss = criterion(estimated_output, expected_output)\n","    loss.backward()\n","    optimizer.step()\n","    optimizer.zero_grad()\n","\n","    train_loss_sum += loss.item()\n","    train_loss = train_loss_sum / (1 + i)\n","    print('\\r', 'epoch: %d, batch: %d, loss: %.4f' \n","          % (epoch + 1, i + 1, train_loss), end=\"\")\n","\n","    del batch, input, loss, expected_output, estimated_output\n","    th.cuda.empty_cache()\n","\n","  model.eval()\n","  valid_loss_sum = 0\n","  valid_loss = 0\n","  \n","  if epoch==0 or (epoch + 1)%10 == 0:\n","    for i, track in enumerate(validationMusdb, 0):\n","      track.chunk_duration = min(30, track.duration)\n","      mono = np.mean(track.audio, axis=1)\n","      song = track.stems.transpose(0, 2, 1)\n","      song = (song - mono.mean()) / mono.std()\n","      streams = th.from_numpy(song).float()\n","\n","      streams = streams.to(device)\n","      expected_output = streams[1:]\n","      input = streams[0]\n","      estimated_output = apply_model(model, input, shifts=apply_shifts)\n","\n","      loss = criterion(estimated_output, expected_output)\n","\n","      valid_loss_sum += loss.item()\n","      valid_loss = valid_loss_sum / (1 + i)\n","\n","      del streams, input, loss, expected_output, estimated_output\n","      th.cuda.empty_cache()\n","  \n","\n","    print('\\r', 'epoch: %d, training loss: %.4f, validation loss: %.4f' \n","          % (epoch + 1, train_loss, valid_loss))\n","    \n","  else:  \n","    print('\\r', 'epoch: %d, training loss: %.4f' \n","            % (epoch + 1, train_loss))\n","  \n","  if (epoch + 1)%10 == 0:\n","    th.save({\n","      'epoch': epoch+1,\n","      'model_state_dict': model.state_dict(),\n","      'optimizer_state_dict': optimizer.state_dict(),\n","      }, Path(model_path))\n","\n","\n","print('Finished Training')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tnAPLEzB4fEQ"},"source":["This cell shows the output of my last training. \n","\n","```\n"," epoch: 1, training loss: 0.2445, validation loss: 0.1931\n"," epoch: 2, training loss: 0.1871\n"," epoch: 3, training loss: 0.1759\n"," epoch: 4, training loss: 0.1704\n"," epoch: 5, training loss: 0.1629\n"," epoch: 6, training loss: 0.1570\n"," epoch: 7, training loss: 0.1543\n"," epoch: 8, training loss: 0.1492\n"," epoch: 9, training loss: 0.1475\n"," epoch: 10, training loss: 0.1431, validation loss: 0.1424\n"," epoch: 11, training loss: 0.1448\n"," epoch: 12, training loss: 0.1425\n"," epoch: 13, training loss: 0.1373\n"," epoch: 14, training loss: 0.1386\n"," epoch: 15, training loss: 0.1378\n"," epoch: 16, training loss: 0.1344\n"," epoch: 17, training loss: 0.1329\n"," epoch: 18, training loss: 0.1307\n"," epoch: 19, training loss: 0.1264\n"," epoch: 20, training loss: 0.1263, validation loss: 0.1419\n"," epoch: 21, training loss: 0.1249\n"," epoch: 22, training loss: 0.1261\n"," epoch: 23, training loss: 0.1240\n"," epoch: 24, training loss: 0.1196\n"," epoch: 25, training loss: 0.1205\n"," epoch: 26, training loss: 0.1186\n"," epoch: 27, training loss: 0.1187\n"," epoch: 28, training loss: 0.1156\n"," epoch: 29, training loss: 0.1161\n"," epoch: 30, training loss: 0.1172, validation loss: 0.1230\n"," epoch: 31, training loss: 0.1163\n"," epoch: 32, training loss: 0.1168\n"," epoch: 33, training loss: 0.1159\n"," epoch: 34, training loss: 0.1113\n"," epoch: 35, training loss: 0.1128\n"," epoch: 36, training loss: 0.1115\n"," epoch: 37, training loss: 0.1110\n"," epoch: 38, training loss: 0.1095\n"," epoch: 39, training loss: 0.1098\n"," epoch: 40, training loss: 0.1080, validation loss: 0.1136\n"," epoch: 41, training loss: 0.1081\n"," epoch: 42, training loss: 0.1085\n"," epoch: 43, training loss: 0.1101\n"," epoch: 44, training loss: 0.1062\n"," epoch: 45, training loss: 0.1044\n"," epoch: 46, training loss: 0.1071\n"," epoch: 47, training loss: 0.1077\n"," epoch: 48, training loss: 0.1044\n"," epoch: 49, training loss: 0.1048\n"," epoch: 50, training loss: 0.1042, validation loss: 0.1191\n"," epoch: 51, training loss: 0.1049\n"," epoch: 52, training loss: 0.1043\n"," epoch: 53, training loss: 0.1035\n"," epoch: 54, training loss: 0.1036\n"," epoch: 55, training loss: 0.1016\n"," epoch: 56, training loss: 0.1018\n"," epoch: 57, training loss: 0.1007\n"," epoch: 58, training loss: 0.1013\n"," epoch: 59, training loss: 0.1008\n"," epoch: 60, training loss: 0.1016, validation loss: 0.1179\n"," epoch: 61, training loss: 0.1018\n"," epoch: 62, training loss: 0.0991\n"," epoch: 63, training loss: 0.1001\n"," epoch: 64, training loss: 0.0991\n"," epoch: 65, training loss: 0.0988\n"," epoch: 66, training loss: 0.0994\n"," epoch: 67, training loss: 0.0991\n"," epoch: 68, training loss: 0.0994\n"," epoch: 69, training loss: 0.0958\n"," epoch: 70, training loss: 0.0970, validation loss: 0.1119\n"," epoch: 71, training loss: 0.0965\n"," epoch: 72, training loss: 0.0950\n"," epoch: 73, training loss: 0.0948\n"," epoch: 74, training loss: 0.0946\n"," epoch: 75, training loss: 0.0954\n"," epoch: 76, training loss: 0.0949\n"," epoch: 77, training loss: 0.0952\n"," epoch: 78, training loss: 0.0942\n"," epoch: 79, training loss: 0.0936\n"," epoch: 80, training loss: 0.0937, validation loss: 0.1038\n"," epoch: 81, training loss: 0.0937\n"," epoch: 82, training loss: 0.0927\n"," epoch: 83, training loss: 0.0920\n"," epoch: 84, training loss: 0.0899\n"," epoch: 85, training loss: 0.0922\n"," epoch: 86, training loss: 0.0921\n"," epoch: 87, training loss: 0.0910\n"," epoch: 88, training loss: 0.0915\n"," epoch: 89, training loss: 0.0894\n"," epoch: 90, training loss: 0.0915, validation loss: 0.1026\n"," epoch: 91, training loss: 0.0891\n"," epoch: 92, training loss: 0.0911\n"," epoch: 93, training loss: 0.0886\n"," epoch: 94, training loss: 0.0920\n"," epoch: 95, training loss: 0.0910\n"," epoch: 96, training loss: 0.0890\n"," epoch: 97, training loss: 0.0900\n"," epoch: 98, training loss: 0.0892\n"," epoch: 99, training loss: 0.0885\n"," epoch: 100, training loss: 0.0859, validation loss: 0.1091\n"," epoch: 101, training loss: 0.0887\n"," epoch: 102, training loss: 0.0883\n"," epoch: 103, training loss: 0.0863\n"," epoch: 104, training loss: 0.0876\n"," epoch: 105, training loss: 0.0868\n"," epoch: 106, training loss: 0.0870\n"," epoch: 107, training loss: 0.0879\n"," epoch: 108, training loss: 0.0856\n"," epoch: 109, training loss: 0.0845\n"," epoch: 110, training loss: 0.0842, validation loss: 0.1080\n"," epoch: 111, training loss: 0.0847\n"," epoch: 112, training loss: 0.0836\n"," epoch: 113, training loss: 0.0866\n"," epoch: 114, training loss: 0.0832\n"," epoch: 115, training loss: 0.0833\n"," epoch: 116, training loss: 0.0848\n"," epoch: 117, training loss: 0.0853\n"," epoch: 118, training loss: 0.0837\n"," epoch: 119, training loss: 0.0828\n"," epoch: 120, training loss: 0.0837, validation loss: 0.1051\n"," epoch: 121, training loss: 0.0817\n"," epoch: 122, training loss: 0.0817\n"," epoch: 123, training loss: 0.0807\n"," epoch: 124, training loss: 0.0822\n"," epoch: 125, training loss: 0.0820\n"," epoch: 126, training loss: 0.0805\n"," epoch: 127, training loss: 0.0801\n"," epoch: 128, training loss: 0.0802\n"," epoch: 129, training loss: 0.0811\n"," epoch: 130, training loss: 0.0798, validation loss: 0.1001\n"," epoch: 131, training loss: 0.0803\n"," epoch: 132, training loss: 0.0797\n"," epoch: 133, training loss: 0.0799\n"," epoch: 134, training loss: 0.0821\n"," epoch: 135, training loss: 0.0793\n"," epoch: 136, training loss: 0.0783\n"," epoch: 137, training loss: 0.0797\n"," epoch: 138, training loss: 0.0802\n"," epoch: 139, training loss: 0.0785\n"," epoch: 140, training loss: 0.0791, validation loss: 0.0907\n"," epoch: 141, training loss: 0.0792\n"," epoch: 142, training loss: 0.0791\n"," epoch: 143, training loss: 0.0781\n"," epoch: 144, training loss: 0.0784\n"," epoch: 145, training loss: 0.0783\n"," epoch: 146, training loss: 0.0777\n"," epoch: 147, training loss: 0.0773\n"," epoch: 148, training loss: 0.0761\n"," epoch: 149, training loss: 0.0757\n"," epoch: 150, training loss: 0.0752, validation loss: 0.0906\n"," epoch: 151, training loss: 0.0775\n"," epoch: 152, training loss: 0.0776\n"," epoch: 153, training loss: 0.0779\n"," epoch: 154, training loss: 0.0765\n"," epoch: 155, training loss: 0.0757\n"," epoch: 156, training loss: 0.0748\n"," epoch: 157, training loss: 0.0758\n"," epoch: 158, training loss: 0.0765\n"," epoch: 159, training loss: 0.0762\n"," epoch: 160, training loss: 0.0758, validation loss: 0.0960\n"," epoch: 161, training loss: 0.0755\n"," epoch: 162, training loss: 0.0742\n"," epoch: 163, training loss: 0.0753\n"," epoch: 164, training loss: 0.0767\n"," epoch: 165, training loss: 0.0753\n"," epoch: 166, training loss: 0.0737\n"," epoch: 167, training loss: 0.0744\n"," epoch: 168, training loss: 0.0743\n"," epoch: 169, training loss: 0.0743\n"," epoch: 170, training loss: 0.0767, validation loss: 0.0988\n"," epoch: 171, training loss: 0.0746\n"," epoch: 172, training loss: 0.0726\n"," epoch: 173, training loss: 0.0722\n"," epoch: 174, training loss: 0.0738\n"," epoch: 175, training loss: 0.0733\n"," epoch: 176, training loss: 0.0720\n"," epoch: 177, training loss: 0.0723\n"," epoch: 178, training loss: 0.0729\n"," epoch: 179, training loss: 0.0721\n"," epoch: 180, training loss: 0.0727, validation loss: 0.0911\n"," epoch: 181, training loss: 0.0733\n"," epoch: 182, training loss: 0.0730\n"," epoch: 183, training loss: 0.0714\n"," epoch: 184, training loss: 0.0698\n"," epoch: 185, training loss: 0.0714\n"," epoch: 186, training loss: 0.0699\n"," epoch: 187, training loss: 0.0731\n"," epoch: 188, training loss: 0.0720\n"," epoch: 189, training loss: 0.0736\n"," epoch: 190, training loss: 0.0734, validation loss: 0.0890\n"," epoch: 191, training loss: 0.0744\n"," epoch: 192, training loss: 0.0720\n"," epoch: 193, training loss: 0.0702\n"," epoch: 194, training loss: 0.0723\n"," epoch: 195, training loss: 0.0708\n"," epoch: 196, training loss: 0.0717\n"," epoch: 197, training loss: 0.0693\n"," epoch: 198, training loss: 0.0701\n"," epoch: 199, training loss: 0.0689\n"," epoch: 200, training loss: 0.0700, validation loss: 0.0933\n"," epoch: 201, training loss: 0.0702\n"," epoch: 202, training loss: 0.0701\n"," epoch: 203, training loss: 0.0707\n"," epoch: 204, training loss: 0.0705\n"," epoch: 205, training loss: 0.0701\n"," epoch: 206, training loss: 0.0680\n"," epoch: 207, training loss: 0.0674\n"," epoch: 208, training loss: 0.0677\n"," epoch: 209, training loss: 0.0704\n"," epoch: 210, training loss: 0.0680, validation loss: 0.0792\n"," epoch: 211, training loss: 0.0697\n"," epoch: 212, training loss: 0.0702\n"," epoch: 213, training loss: 0.0691\n"," epoch: 214, training loss: 0.0679\n"," epoch: 215, training loss: 0.0700\n"," epoch: 216, training loss: 0.0702\n"," epoch: 217, training loss: 0.0683\n"," epoch: 218, training loss: 0.0669\n"," epoch: 219, training loss: 0.0676\n"," epoch: 220, training loss: 0.0669, validation loss: 0.0736\n"," epoch: 221, training loss: 0.0678\n"," epoch: 222, training loss: 0.0696\n"," epoch: 223, training loss: 0.0678\n"," epoch: 224, training loss: 0.0677\n"," epoch: 225, training loss: 0.0675\n"," epoch: 226, training loss: 0.0667\n"," epoch: 227, training loss: 0.0688\n"," epoch: 228, training loss: 0.0681\n"," epoch: 229, training loss: 0.0663\n"," epoch: 230, training loss: 0.0673, validation loss: 0.0865\n"," epoch: 231, training loss: 0.0677\n"," epoch: 232, training loss: 0.0650\n"," epoch: 233, training loss: 0.0652\n"," epoch: 234, training loss: 0.0660\n"," epoch: 235, training loss: 0.0664\n"," epoch: 236, training loss: 0.0659\n"," epoch: 237, training loss: 0.0664\n"," epoch: 238, training loss: 0.0652\n"," epoch: 239, training loss: 0.0661\n"," epoch: 240, training loss: 0.0665, validation loss: 0.0772\n","Finished Training\n","```\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"lBzbLBKNDeRT"},"source":["Example test"]},{"cell_type":"code","metadata":{"id":"vVBwXOimDfVo"},"source":["'''\n","This cell applies the model on a track from the validation dataset as a simple\n","test to check the results of the training.\n","'''\n","\n","track = validationMusdb[0]\n","track.chunk_duration = min(30, track.duration)\n","\n","print(\"mixture\")\n","display(Audio(track.targets['linear_mixture'].audio.T, rate=44100))\n","\n","print(\"expected output\")\n","display(Audio(track.targets['drums'].audio.T, rate=44100))\n","display(Audio(track.targets['bass'].audio.T, rate=44100))\n","display(Audio(track.targets['other'].audio.T, rate=44100))\n","display(Audio(track.targets['vocals'].audio.T, rate=44100))\n","\n","mono = np.mean(track.audio, axis=1)\n","song = track.stems.transpose(0, 2, 1)\n","song = (song - mono.mean()) / mono.std()\n","streams = th.from_numpy(song).float()\n","streams = streams.to(device)\n","\n","output = apply_model(model, streams[0], shifts=apply_shifts)\n","output = output * mono.std() + mono.mean()\n","\n","print(\"estimated output\")\n","for target in output:\n","  display(Audio(target.cpu().detach().numpy(), rate=44100))\n","\n","del streams, output\n","th.cuda.empty_cache() "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HzPfklUBPeqH"},"source":["Save test estimates and compute loss"]},{"cell_type":"code","metadata":{"id":"efBEvGnpPhvf"},"source":["'''\n","This cell applies the model on all the mixtures in the test dataset, saves the\n","estimates in the google drive and computes the average loss.\n","'''\n","\n","#del training_set\n","test_set = musdb.DB(Path(\"musdb\"), subsets=[\"test\"])\n","model.eval()\n","for p in model.parameters():\n","  p.requires_grad = False\n","  p.grad = None\n","\n","test_loss_sum = 0\n","test_loss = 0\n","\n","for i, track in enumerate(test_set):\n","  print('\\r', '%d/%d' % (i + 1, len(test_set)), end = \"\")\n","\n","  mono = np.mean(track.audio, axis=1)\n","  song = track.stems.transpose(0, 2, 1)\n","  song = (song - mono.mean()) / mono.std()\n","\n","  streams = th.from_numpy(song).float().to(device)\n","  reference = streams[1:]\n","  input = streams[0]\n","\n","  output = apply_model(model, input, shifts=apply_shifts, split=True)\n","\n","  loss = criterion(output, reference)\n","  test_loss_sum += loss.item()\n","  test_loss = test_loss_sum / (1 + i)\n","\n","  print('\\r', '%d/%d, loss: %.4f' % (i + 1, len(test_set), test_loss), end=\"\")\n","\n","  output = output * mono.std() + mono.mean()\n","\n","  estimates = output.cpu().numpy().transpose(0, 2, 1)\n","  estimates_dict = {\n","    \"drums\": estimates[0], \n","    \"bass\": estimates[1], \n","    \"other\": estimates[2], \n","    \"vocals\": estimates[3], \n","  }\n","  test_set.save_estimates(estimates_dict, track, Path(estimates_path))  \n","\n","  del input, output, loss\n","  th.cuda.empty_cache()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kGtrZaVLKfKy"},"source":["The loss after the last training I did was 0.1027"]}]}